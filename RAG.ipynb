{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTSfMImyEmz3ZQCISq6cLC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lol782/RAG-powered-chatbot-for-Kumaoni-language-preservation/blob/main/RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This is just for testing**"
      ],
      "metadata": {
        "id": "Id17QCx4FdjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "\n",
        "# Load dataset\n",
        "with open(\"kumaoni_euttaranchal_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "# Create smaller chunks with metadata\n",
        "docs = []\n",
        "for i, item in enumerate(raw_data):\n",
        "    if isinstance(item.get(\"english\"), str) and isinstance(item.get(\"kumaoni\"), str):\n",
        "        text = f\"English: {item['english']} | Kumaoni: {item['kumaoni']}\"\n",
        "        doc = Document(page_content=text, metadata={\"source\": f\"entry_{i}\"})\n",
        "        docs.append(doc)\n",
        "\n",
        "# Optional: Split longer chunks (if sentences are long)\n",
        "text_splitter = CharacterTextSplitter(separator=\"|\", chunk_size=256, chunk_overlap=20)\n",
        "split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "# Embed using Gemini embeddings\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(split_docs, embedding)\n",
        "vectorstore.save_local(\"kumaoni_faiss_index\")\n"
      ],
      "metadata": {
        "id": "lOCCfgaoO2w4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d14beeb-ed2f-4f12-8e41-324414581d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 290, which is longer than the specified 256\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 297, which is longer than the specified 256\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 309, which is longer than the specified 256\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 362, which is longer than the specified 256\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 400, which is longer than the specified 256\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 324, which is longer than the specified 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Load vectorstore & retriever\n",
        "retriever = FAISS.load_local(\"kumaoni_faiss_index\", embedding, allow_dangerous_deserialization=True).as_retriever()\n",
        "retriever.search_kwargs[\"k\"] = 5  # Fetch top 5 relevant chunks\n",
        "\n",
        "# Prompt Template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "You are a helpful and culturally aware Kumaoni assistant. Your goal is to provide friendly, conversational, and complete answers in the **Kumaoni language**.\n",
        "\n",
        "Use the context below to guide your tone and vocabulary, but feel free to respond with natural Kumaoni sentences â€” not just direct translations. You may include greetings or polite phrases to make the conversation feel warm and local.\n",
        "\n",
        "When appropriate, explain things like a local guide or elder would, giving real-life examples from Kumaoni culture, traditions, or places.\n",
        "\n",
        "Always reply only in Kumaoni.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User: {question}\n",
        "Kumaoni Response (Only answer in Kumaoni, no explanation):\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Gemini LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash\", temperature=0.4)\n",
        "\n",
        "# RetrievalQA with Sources (for debugging/testing)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n"
      ],
      "metadata": {
        "id": "RGwcajWCRBHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\"]:\n",
        "        break\n",
        "    response = qa_chain.run(user_input)\n",
        "    print(\"Kumaoni Bot:\", response)\n"
      ],
      "metadata": {
        "id": "dhsI77cWRO5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok nest_asyncio python-dotenv\n"
      ],
      "metadata": {
        "id": "B5vjwTIn_ueg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "id": "e5yhIRNWAEgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uvicorn\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start ngrok tunnel for port 8000\n",
        "ngrok.set_auth_token(\"2uXZquOIfZsWQskGbFfztHHOjjy_4czZ4azSRn6AUv9kLZew2\")  # Replace with your ngrok token\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Start FastAPI server\n",
        "# The first argument to uvicorn.run should be the FastAPI app object, not the file name.\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000) # Changed app.py to app"
      ],
      "metadata": {
        "id": "tpa15775AHMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ca70498"
      },
      "source": [
        "## Create a fastapi application\n",
        "\n",
        "### Subtask:\n",
        "Define a FastAPI app instance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ca7b27b"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the FastAPI class and create an instance of the FastAPI application.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "879d0642"
      },
      "source": [
        "from fastapi import FastAPI\n",
        "\n",
        "app = FastAPI()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "761be5ef"
      },
      "source": [
        "## Define a pydantic model for input\n",
        "\n",
        "### Subtask:\n",
        "Create a Pydantic model to validate the incoming request data, which will be the user's query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "279cd40f"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a Pydantic model to validate the incoming request data based on the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2f1c547"
      },
      "source": [
        "from pydantic import BaseModel\n",
        "\n",
        "class Query(BaseModel):\n",
        "    user_query: str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae5992d3"
      },
      "source": [
        "## Create an api endpoint\n",
        "\n",
        "### Subtask:\n",
        "Define an asynchronous POST endpoint that accepts the user query using the defined Pydantic model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52991933"
      },
      "source": [
        "**Reasoning**:\n",
        "Define an asynchronous POST endpoint that accepts the user query using the defined Pydantic model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8804eab"
      },
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Query(BaseModel):\n",
        "    user_query: str\n",
        "\n",
        "@app.post(\"/kumaoni_query\")\n",
        "async def kumaoni_query_endpoint(query: Query):\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7d7c3a7"
      },
      "source": [
        "## Integrate langchain components\n",
        "\n",
        "### Subtask:\n",
        "Inside the API endpoint, load the necessary Langchain components (retriever, LLM, QA chain) and use them to process the user query.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a15d9f23"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the logic within the API endpoint to load Langchain components and process the user query.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a6fd914"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `langchain_community` was not found. Although `langchain-community` was installed in a previous cell, the current runtime might not have it available. Re-installing it within this cell will ensure it's available for the code to run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-l7199W7AyPt"
      },
      "source": [
        "!pip install -U langchain-community"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de49694a"
      },
      "source": [
        "**Reasoning**:\n",
        "Re-execute the code to load the necessary Langchain components and define the API endpoint, now that `langchain-community` is confirmed to be installed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "611b4afb"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `langchain_google_genai` was not found. Re-installing it within this cell will ensure it's available for the code to run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZPI7XrfA4Ir"
      },
      "source": [
        "!pip install -U langchain_google_genai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a8e8238"
      },
      "source": [
        "**Reasoning**:\n",
        "Re-execute the code to load the necessary Langchain components and define the API endpoint, now that `langchain_google_genai` is confirmed to be installed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e0350b2"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `faiss` package was not found, despite attempts to install `faiss-cpu` and `langchain-community`. Re-installing `faiss-cpu` explicitly within this cell will ensure it's available for the code to run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoNgaH-tBE-S"
      },
      "source": [
        "!pip install -U faiss-cpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a51b83ba"
      },
      "source": [
        "**Reasoning**:\n",
        "Re-execute the code to load the necessary Langchain components and define the API endpoint, now that `faiss-cpu` is confirmed to be installed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afa0842c"
      },
      "source": [
        "import json, os\n",
        "from fastapi import FastAPI\n",
        "from fastapi.middleware.cors import CORSMiddleware   # ðŸ”¹ add\n",
        "from pydantic import BaseModel\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import (GoogleGenerativeAIEmbeddings,\n",
        "                                    ChatGoogleGenerativeAI)\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio, uvicorn\n",
        "\n",
        "# â”€â”€ API key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n",
        "\n",
        "# â”€â”€ FastAPI app â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "app = FastAPI()\n",
        "\n",
        "# ðŸ”¹  CORS middleware  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],            # â† in prod, replace * with your site\n",
        "    allow_methods=[\"*\"],            # allow POST / OPTIONS\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# â”€â”€ Pydantic model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "class Query(BaseModel):\n",
        "    user_query: str\n",
        "\n",
        "# â”€â”€ Build / load vector index (runs once) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "with open(\"kumaoni_euttaranchal_dataset.json\", encoding=\"utf-8\") as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "docs = [\n",
        "    Document(page_content=f\"English: {item['english']} | Kumaoni: {item['kumaoni']}\")\n",
        "    for item in raw\n",
        "    if isinstance(item.get(\"english\"), str) and isinstance(item.get(\"kumaoni\"), str)\n",
        "]\n",
        "\n",
        "emb = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "index_path = \"kumaoni_faiss_index\"\n",
        "\n",
        "if os.path.exists(index_path):\n",
        "    vectorstore = FAISS.load_local(index_path, emb, allow_dangerous_deserialization=True)\n",
        "else:\n",
        "    vectorstore = FAISS.from_documents(docs, emb)\n",
        "    vectorstore.save_local(index_path)\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# â”€â”€ Prompt & chain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "template = \"\"\"\n",
        "You are a helpful and culturally aware Kumaoni assistant. Your goal is to provide friendly, conversational, and complete answers in the **Kumaoni language**.\n",
        "\n",
        "Use the context below to guide your tone and vocabulary, but feel free to respond with natural Kumaoni sentences â€” not just direct translations. You may include greetings or polite phrases to make the conversation feel warm and local.\n",
        "\n",
        "When appropriate, explain things like a local guide or elder would, giving real-life examples from Kumaoni culture, traditions, or places.\n",
        "\n",
        "Always reply only in Kumaoni.\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "User: {question}\n",
        "Kumaoni Response:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
        "\n",
        "llm   = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash\", temperature=0.4)\n",
        "chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt},\n",
        ")\n",
        "\n",
        "# â”€â”€ Endpoint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@app.post(\"/kumaoni_query\")\n",
        "async def kumaoni_query(q: Query):\n",
        "    return {\"answer\": chain.run(q.user_query)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b547de6"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the return statement in the `kumaoni_query_endpoint` function to return a dictionary containing the Kumaoni response, which FastAPI will serialize into JSON.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0206d224"
      },
      "source": [
        "## Update the uvicorn run command\n",
        "\n",
        "### Subtask:\n",
        "Modify the uvicorn run command to use the newly created FastAPI app instance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fd5456a"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the uvicorn run command to use the newly created FastAPI app instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acb0f86d"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `pyngrok` module was not found. I need to install `pyngrok` to fix this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D0YHwy-BfLH",
        "outputId": "ae60908f-62df-41ee-aead-4450fdac8016"
      },
      "source": [
        "!pip install pyngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2bb888c"
      },
      "source": [
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start ngrok tunnel for port 8000\n",
        "ngrok.set_auth_token(\"********\")  # Replace with your ngrok token\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "# Start FastAPI server\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}